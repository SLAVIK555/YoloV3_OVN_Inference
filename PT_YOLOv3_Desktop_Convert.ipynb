{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGDY2HTyjoqm"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive #for colab\n",
    "# drive.mount('/content/gdrive')\n",
    "# !ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "# !ls /mydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWQk3vp_kGyV"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov3.git #for colab\n",
    "# %cd yolov3\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xzyhar-apLMb"
   },
   "outputs": [],
   "source": [
    "# path = \"/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/320-200\"\n",
    "# mname = \"best\"\n",
    "# path_to_pt_model = path + \"/\" + mname + \".pt\"\n",
    "# path_to_onnx_model = path + \"/\" + mname + \".onnx\"\n",
    "# img = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vBdRikowq_dt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=../../Repos/yolov3/data/coco128.yaml, weights=/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.pt, imgsz=[416], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=13, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv3 ðŸš€ v9.6.0-7-g0519223 torch 1.10.1+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 48 layers, 8671312 parameters, 0 gradients, 12.9 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.pt (17.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.10.2...\n",
      "/home/slava/Repos/yolov3/models/yolo.py:58: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success, saved as /home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.onnx (34.7 MB)\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m run --dynamic ONNX model inference with: 'python detect.py --weights /home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.onnx'\n",
      "\n",
      "Export complete (1.26s)\n",
      "Results saved to \u001b[1m/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800\u001b[0m\n",
      "Visualize with https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/slava/Repos/yolov3/export.py --weights \"/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.pt\" --imgsz 416 --batch-size 1 --include onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jKvCS64_rZ5A",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.onnx\n",
      "\t- Path for generated IR: \t/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800\n",
      "\t- IR output name: \tbest\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \timages\n",
      "\t- Output layers: \tConv_56,Conv_110\n",
      "\t- Input shapes: \t[1,3,416,416]\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \t255.0\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "\t- Inference Engine found in: \t/opt/intel/openvino_2021.4.752/python/python3.6/openvino\n",
      "Inference Engine version: \t2021.4.2-3974-e2a469a3450-releases/2021/4\n",
      "Model Optimizer version: \t2021.4.2-3974-e2a469a3450-releases/2021/4\n",
      "Progress: [................... ]  98.00% done[ WARNING ]  Const node 'Resize_51/Add_input_port_1/value6692704' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "Progress: [....................] 100.00% done[ WARNING ]  Changing Const node 'Resize_51/Add_input_port_1/value6692756' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.xml\n",
      "[ SUCCESS ] BIN file: /home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.bin\n",
      "[ SUCCESS ] Total execution time: 4.08 seconds. \n",
      "[ SUCCESS ] Memory consumed: 207 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n"
     ]
    }
   ],
   "source": [
    "!python3 /opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py --input_model \"/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800/best.onnx\" --output_dir \"/home/slava/Source/YoloV3_OVN_Inference/PT_V3_models/416tiny-800\" --input_shape [1,3,416,416] --input images --scale 255 --data_type FP16 --output Conv_56,Conv_110 --model_name best --progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMorK0uIrIEBdrG3swk6H7y",
   "collapsed_sections": [],
   "name": "PT_YOLOv3_Colab_Convert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
